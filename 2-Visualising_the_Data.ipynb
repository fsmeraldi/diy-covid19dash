{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will guide you through visualising the data you downloaded through the API in the form of graphs. In the process we introduce ```pandas```, a very popular data processing library. Entire books have been written on the  ```pandas``` library, and there is an extensive online [documentation](https://pandas.pydata.org/docs/); in this notebook, we will restrict ourselves to the minimum required to use the library as a wrapper for ```matplotlib```, a lower-level data visualisation library.\n",
    "\n",
    "Both ```pandas``` and ```matplotlib``` are installed on the QMUL JupyterHub server, and they are likely to be part of any major Python distribution, so the following ```import``` statements should just work. Installation instructions for [pandas](https://pandas.pydata.org/docs/getting_started/install.html) and [matplotlib](https://matplotlib.org/users/installing.html) are available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an iPython  \"magic\" that enables the embedding of matplotlib output\n",
    "%matplotlib inline\n",
    "# make figures larger\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data from the JSON file in which we saved it. Reading a JSON file in Python is not any more difficult than writing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"admissions.json\", \"rt\") as INFILE:\n",
    "    admissions=json.load(INFILE)\n",
    "with open(\"cases.json\", \"rt\") as INFILE:\n",
    "    cases=json.load(INFILE)\n",
    "with open(\"deaths.json\", \"rt\") as INFILE:\n",
    "    deaths=json.load(INFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling the data\n",
    "\n",
    "Whatever your query, the data is unlikely to come in a form that's amenable to be plotted directly; some wrangling, munging, or other bit of cool data science jargon is required. Our aim is to extract values for the *x* axis of our plot (in this case the dates). These will become the *index* of a ```DataFrame```, of which the *columns* will contain the data proper. You can, for the moment, think of a ```DataFrame``` as the Python equivalent of an Excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data consists of a list of dictionaries; actual data is stored in the inner dictionaries under the ```date``` and ```metric_value``` keys. Let us start by retrieving those values and bringing everything together into a dictionary with the dates as keys, and inner dictionaries with the three metrics as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for dataset in [admissions, cases, deaths]:\n",
    "    for entry in dataset:\n",
    "        date=entry['date']\n",
    "        metric=entry['metric']\n",
    "        value=entry['metric_value']\n",
    "        if date not in data:\n",
    "            data[date]={}\n",
    "        data[date][metric]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the metrics are missing for some dates, and the keys ate not sorted by date, but by insertion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2020-01-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to extract all the dates and sort them. Luckily, with dates written year-first, alphabetical ordering does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=list(data.keys())\n",
    "dates.sort()\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not assume this is a proper time series: some dates might be missing. At any rate, we find the earliest and latest date and convert them to the ```pandas``` type for representing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(datestring):\n",
    "    \"\"\" Convert a date string into a pandas datetime object \"\"\"\n",
    "    return pd.to_datetime(datestring, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate=parse_date(dates[0])\n",
    "enddate=parse_date(dates[-1])\n",
    "print (startdate, ' to ', enddate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our ```DataFrame```. To start with, we create an index as a ```date_range```: this is the date analog of a ```range``` for integers, and it will include any dates that may be missing from our list.\n",
    "We then proceed to define the ```DateFrame``` by specifying its index and the title of its columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pd.date_range(startdate, enddate, freq='D')\n",
    "timeseriesdf=pd.DataFrame(index=index, columns=['cases', 'admissions', 'deaths'])\n",
    "timeseriesdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ```DataFrame``` resembles a spreadsheet, or if you want a matrix with labels. We now proceed to fill it in with values from our three statistics. Note that there are many ways of filling in a ```DataFrame```: this is probably not the most efficient, but it is in my view the easiest to understand and one of those that give you more control over the values you enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the columns to our metrics\n",
    "metrics ={'cases': 'COVID-19_cases_casesByDay',\n",
    "          'admissions': 'COVID-19_healthcare_admissionByDay',\n",
    "          'deaths': 'COVID-19_deaths_ONSByDay'}\n",
    "\n",
    "for date, entry in data.items(): # each entry is a dictionary with cases, admissions and deaths\n",
    "    pd_date=parse_date(date) # convert to Pandas format\n",
    "    for column in ['cases', 'admissions', 'deaths']: \n",
    "        metric_name=metrics[column]\n",
    "        # do not assume all values are there for every date - if a value is not available, insert a 0.0\n",
    "        value= entry.get(metric_name, 0.0)\n",
    "        # this is the way you access a specific location in the dataframe - use .loc\n",
    "        # and put index,column in a single set of [ ]\n",
    "        timeseriesdf.loc[date, column]=value\n",
    "            \n",
    "# fill in any remaining \"holes\" due to missing dates\n",
    "timeseriesdf.fillna(0.0, inplace=True)\n",
    "            \n",
    "timeseriesdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basic plot of our data is now dead easy. We just need to call the ```plot()``` method on our ```DataFrame```; this will call the underlying ```matplot.pyplot``` functions to create a plot of the data. For more options and fancier graphs, see the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=timeseriesdf.plot() # easy peasy...\n",
    "ax.set_title('Daily cases, hospital admissions and deaths');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After lording it for centuries over engineering calculations up to the advent of computers, logarithms had never been so popular as they have become since the rise of Covid dashboards - now everybody and their uncle knows what a log scale is. There is a reason for that: exponential phenomena are well suited for plotting on log scales, where an exponential propagation becomes a straight line. You wouldn't want to miss out on this centuries-old fad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=timeseriesdf.plot(logy=True) # ...lemon squeezy\n",
    "ax.set_title('Daily cases, hospital admissions and deaths (log scale)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second example, let's lineage data. The data scructure, as you can see, is a bit different, so this will be another good exercise in data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lineage.json\", \"rt\") as INFILE:\n",
    "    lineage=json.load(INFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data munging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, there are no fixed rules for this - just see in what format your data comes, have clear in your mind what you are aiming for, and wrangle your data accordingly. In this case, we again want to get to a ```DataFrame``` with the dates as the *index*, and the prevalence of the different variants as its *columns*.\n",
    "\n",
    "In this case, the fields of interest out of this series are ```date```, ```stratum``` and ```metric_value```. Notice also that for each ```date``` there are several entries, one for each variant represented in that period. Let us again try to organise the data into a dictionary of dictionaries, with the dates as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for entry in lineage:\n",
    "    date=entry['date']\n",
    "    variant=entry['stratum']\n",
    "    value=entry['metric_value']\n",
    "    if date not in data:\n",
    "        data[date]={}\n",
    "    data[date][variant]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the dates works as above. Notice that the dates are at least one week apart, as this is a weekly metric; in fact, they are all Mondays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=list(data.keys())\n",
    "dates.sort()\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate=parse_date(dates[0])\n",
    "enddate=parse_date(dates[-1])\n",
    "print (startdate, ' to ', enddate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the columns is a bit more complicated, as we do not know their names and they do not all appear at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants=[]\n",
    "for entry in data.values():\n",
    "    for var in entry.keys():\n",
    "        if var not in variants:\n",
    "            variants.append(var)\n",
    "variants.sort()\n",
    "print(variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to define our ```DataFrame```. We use a date range as an *index*, but this time we have to create with a weekly spacing. The sorted variants will be the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W-MON is \"weekly on Mondays\"\n",
    "index=pd.date_range(startdate, enddate, freq='W-MON')\n",
    "lineagedf=pd.DataFrame(index=index, columns=variants)\n",
    "lineagedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in ```lineagedf``` is only a matter of looping over the dates and filling in the columns with the corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, entry in data.items(): # each entry is a dic with some variants and values\n",
    "    pd_date=parse_date(date) # convert to Pandas format\n",
    "    for column in entry.keys(): # the variants\n",
    "        # this is the way you access a specific location in the dataframe - use .loc\n",
    "        # and put index,column in a single set of [ ]\n",
    "        lineagedf.loc[date, column]=entry[column]\n",
    "            \n",
    "# fill in any remaining \"holes\" due to missing dates and variants\n",
    "lineagedf.fillna(0.0, inplace=True)\n",
    "            \n",
    "lineagedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data as a multiple stacked bar plot, aggregated by quarter. Each bar has the same meaning as a pie chart, representing proportions, and we have a series them arranged by date. For this and fancier bar plots (and other plots), see the excellent [book](https://link.springer.com/book/10.1007/978-3-031-57051-3) on data visualisation by F. Bianconi. This [tutorial](https://www.shanelynn.ie/bar-plots-in-python-using-pandas-dataframes/) is also useful.\n",
    "\n",
    "To begin with, we group the dataset by quarters by averaging, in order to reduce the amount of data and at the same time deal with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Q means \"by quarter\"; compute the average row\n",
    "quarterly= lineagedf.groupby(pd.Grouper(freq='1Q')).mean() \n",
    "# since some data are missing, the sum across an average row may be less than one\n",
    "totals=quarterly.sum(axis=1) # sum over the rows\n",
    "# make sure it's all normalised to 100\n",
    "quarterly=quarterly.div(totals, axis=0)*100 # divide the columns by the totals\n",
    "# reverse the rows for the graph, so older dates will be on top\n",
    "quarterly = quarterly[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting is now relatively straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a colormap with more than 11 colours \n",
    "ax=quarterly.plot(kind='barh', stacked=True,cmap='tab20')\n",
    "ax.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "# format the dates nicely\n",
    "ax.set_yticklabels(quarterly.index.strftime('%Y-%m-%d'))\n",
    "ax.set_title('Lineage prevalence by quarter ending');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get ~~yourself~~ your data into a pickle (file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual dashboard, at this point, there is no need to save the dataframes - you can just add interactive controls to the graphs as you plot them. While developing, however, we want to save the wrangled data in order to be able to experiment with the controls. Pandas luckily makes it easy by providing a method that saves a ```DataFrame``` to a \"pickle\" file. This is based on the ```pickle``` [library](https://docs.python.org/3/library/pickle.html), that however is imported by ```pandas```, so you do not need to import it yourself. While JSON is a standard format that is language-independent, a *pickle* file is Python-specific. This format is thus more flexible, allowing the [serialisation](https://en.wikipedia.org/wiki/Serialization) of a broader range of Python objects; however, it is less portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas makes saving to a pickle file dead easy:\n",
    "timeseriesdf.to_pickle(\"timeseriesdf.pkl\")\n",
    "lineagedf.to_pickle(\"lineagedf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Load the data you saved in the JSON files, munge it, turn it into a ```DataFrame``` and display it; then save your ```DataFrame``` to a *pickle* file. In the next notebook, when we will show you how to add interactive controls to your graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2020,2024 Fabrizio Smeraldi** ([f.smeraldi@qmul.ac.uk](mailto:f.smeraldi@qmul.ac.uk) - [web](http://www.eecs.qmul.ac.uk/~fabri/)). This notebook is released under the [GNU GPLv3.0 or later](https://www.gnu.org/licenses/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
